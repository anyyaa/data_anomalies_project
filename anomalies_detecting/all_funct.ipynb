{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNmkDpfyHyInsE4Rn3dBQ5G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"3M4SERM-FsNA","executionInfo":{"status":"error","timestamp":1744463045555,"user_tz":-180,"elapsed":436,"user":{"displayName":"Анастасия Григорьева","userId":"12705387961757662291"}},"outputId":"7794d47c-ecef-4a7e-8954-f5ca27cb634a"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-979965646a3f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_duplicate_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_session_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;34m\"\"\"Проверка, что сессии начинаются с 1\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":["import pandas as pd\n","from typing import Dict, List, Union, Optional\n","\n","def check_duplicate_events(df: pd.DataFrame) -> Dict:\n","    \"\"\"Проверка дубликатов событий с одинаковыми ts и event\"\"\"\n","    results = {'duplicates': []}\n","    duplicate_cols = ['ts', 'event']\n","\n","    if 'counter_id' in df.columns:\n","        duplicate_cols.append('counter_id')\n","\n","    duplicates = df[df.duplicated(subset=duplicate_cols, keep=False)]\n","    if not duplicates.empty:\n","        results['duplicates'] = duplicates.sort_values(duplicate_cols).to_dict('records')\n","    return results\n","\n","print(check_duplicate_events(df))\n","def check_session_start(df: pd.DataFrame) -> Dict:\n","    \"\"\"Проверка, что сессии начинаются с 1\"\"\"\n","    results = {'start_errors': []}\n","\n","    if 'randPAS_session_id' not in df.columns:\n","        raise KeyError(\"Отсутствует столбец randPAS_session_id\")\n","\n","    first_events = df.groupby('randPAS_session_id').first()\n","\n","    start_errors = first_events[\n","        (first_events['page_view_order_number'] != 1) |\n","        (first_events['event_order_number'] != 1)\n","    ]\n","\n","    for session_id, row in start_errors.iterrows():\n","        results['start_errors'].append({\n","            'session_id': session_id,\n","            'first_page_view': int(row['page_view_order_number']),\n","            'first_event': int(row['event_order_number'])\n","        })\n","\n","    return results\n","\n","print(ckeck_session_start(df))\n","\n","def check_order_relation(df: pd.DataFrame) -> Dict:\n","    \"\"\"Проверка соотношения page_view и event order numbers\"\"\"\n","    results = {'relation_errors': []}\n","\n","    relation_errors = df[df['event_order_number'] < df['page_view_order_number']]\n","\n","    if not relation_errors.empty:\n","        for session_id, group in relation_errors.groupby('randPAS_session_id'):\n","            results['relation_errors'].append({\n","                'session_id': session_id,\n","                'count': len(group),\n","                'examples': group[['ts', 'page_view_order_number', 'event_order_number']].head(3).to_dict('records')\n","            })\n","\n","    return results\n","\n","\n","\n","def check_numbering_sequence(df: pd.DataFrame) -> Dict:\n","    \"\"\"Проверка пропусков в нумерации событий\"\"\"\n","    results = {'missing_numbers': []}\n","\n","    grouped = df.groupby('randPAS_session_id')\n","\n","    for session_id, group in grouped:\n","        session_data = group.sort_values('ts')\n","\n","        page_diff = session_data['page_view_order_number'].diff().dropna()\n","        if any(page_diff != 1):\n","            results['missing_numbers'].append({\n","                'session_id': session_id,\n","                'type': 'page_view',\n","                'positions': session_data[page_diff != 1][['ts', 'page_view_order_number']].to_dict('records')\n","            })\n","\n","\n","        event_diff = session_data['event_order_number'].diff().dropna()\n","        if any(event_diff != 1):\n","            results['missing_numbers'].append({\n","                'session_id': session_id,\n","                'type': 'event',\n","                'positions': session_data[event_diff != 1][['ts', 'event_order_number']].to_dict('records')\n","            })\n","\n","    return results\n","\n","\n","!gdown --id 1GvWVG9iS3sQkYnaERcd_G2hCbpa2xLyC\n","import pandas as pd\n","\n","df = pd.read_parquet('data_2024-10-09_part2.parquet')\n","\n","\n","import pandas as pd\n","from datetime import timedelta\n","\n","def detect_location_changes(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Анализирует смену местоположения (geo_city_id и ip) для каждого пользователя.\n","    Возвращает DataFrame с информацией о сменах местоположения и временных интервалах.\n","    \"\"\"\n","    df_sorted = df.sort_values(['randPAS_user_passport_id', 'ts'])\n","    grouped = df_sorted.groupby('randPAS_user_passport_id')\n","\n","    results = []\n","\n","    for user_id, group in grouped:\n","        user_data = group[['ts', 'ip', 'geo_city_id']].drop_duplicates()\n","\n","        changes = user_data[\n","            (user_data['geo_city_id'].shift() != user_data['geo_city_id']) |\n","            (user_data['ip'].shift() != user_data['ip'])\n","        ].copy()\n","        if len(changes) > 1:\n","            changes['time_diff'] = changes['ts'].diff().dt.total_seconds()\n","            city_changes = list(zip(\n","                changes['geo_city_id'].astype(str),\n","                changes['time_diff'].astype(str)\n","            ))\n","\n","            results.append({\n","                'user_id': user_id,\n","                'city_changes': \" → \".join([f\"{city} ({time}s)\" for city, time in city_changes]),\n","                'change_count': len(changes) - 1,\n","                'first_change': changes['ts'].iloc[1],\n","                'last_change': changes['ts'].iloc[-1],\n","                'unique_cities': changes['geo_city_id'].nunique(),\n","                'unique_ips': changes['ip'].nunique()\n","            })\n","\n","    return pd.DataFrame(results)\n","\n","if __name__ == \"__main__\":\n","\n","    test_data = {\n","        'ts': pd.to_datetime(['2023-01-01 10:00', '2023-01-01 11:00', '2023-01-01 12:00',\n","                             '2023-01-02 09:00', '2023-01-02 10:00']),\n","        'randPAS_user_passport_id': ['user1', 'user1', 'user1', 'user2', 'user2'],\n","        'ip': ['192.168.1.1', '192.168.1.2', '192.168.1.2', '10.0.0.1', '10.0.0.2'],\n","        'geo_city_id': [1, 2, 5, 3, 4]\n","    }\n","    df = pd.DataFrame(test_data)\n","\n","    result_df = detect_location_changes(df)\n","\n","    print(\"Анализ смены местоположения пользователей:\")\n","    print(result_df[['user_id', 'city_changes', 'change_count']])\n","\n","    if not result_df.empty:\n","        print(\"\\nСтатистика по изменениям:\")\n","        print(f\"Всего пользователей с изменениями: {len(result_df)}\")\n","        print(f\"Среднее количество изменений на пользователя: {result_df['change_count'].mean():.2f}\")\n","        print(f\"Максимальное количество городов у одного пользователя: {result_df['unique_cities'].max()}\")\n","\n","\n","import pandas as pd\n","import numpy as np\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","\n","def analyze_city_activity(df, min_events=10, z_threshold=3, rolling_window='1H'):\n","    \"\"\"\n","    Анализирует активность по городам и выявляет аномальные всплески\n","\n","    Параметры:\n","        df: DataFrame с данными\n","        min_events: минимальное количество событий для анализа города\n","        z_threshold: порог для детектирования аномалий (в стандартных отклонениях)\n","        rolling_window: размер окна для скользящей статистики\n","\n","    Возвращает:\n","        DataFrame с результатами анализа\n","    \"\"\"\n","\n","    if not pd.api.types.is_datetime64_any_dtype(df['ts']):\n","        df['ts'] = pd.to_datetime(df['ts'])\n","    city_activity = df.groupby(['geo_city_id', pd.Grouper(key='ts', freq=rolling_window)])\\\n","                     .size()\\\n","                     .reset_index(name='event_count')\n","    #print(city_activity)\n","    city_stats = city_activity.groupby('geo_city_id')['event_count'].agg(['count', 'mean', 'std'])\n","    valid_cities = city_stats[city_stats['count'] > 5].index\n","    city_activity = city_activity[city_activity['geo_city_id'].isin(valid_cities)]\n","\n","    city_activity['z_score'] = city_activity.groupby('geo_city_id')['event_count']\\\n","        .transform(lambda x: (x - x.mean()) / x.std())\n","\n","    city_activity['is_anomaly'] = city_activity['z_score'] > z_threshold\n","\n","\n","    total_activity = df.groupby(pd.Grouper(key='ts', freq=rolling_window))\\\n","                      .size()\\\n","                      .reset_index(name='total_events')\n","\n","    city_activity = city_activity.merge(total_activity, on='ts')\n","    city_activity['activity_ratio'] = city_activity['event_count'] / city_activity['total_events']\n","\n","    city_activity['prev_ratio'] = city_activity.groupby('geo_city_id')['activity_ratio'].shift(1)\n","    city_activity['ratio_change'] = (city_activity['activity_ratio'] - city_activity['prev_ratio']) / city_activity['prev_ratio']\n","\n","    anomalies = city_activity[city_activity['is_anomaly']].sort_values('z_score', ascending=False)\n","\n","    return anomalies, city_activity\n","\n","\n","\n","anomalies, city_activity = analyze_city_activity(df)\n","\n","print(\"Топ аномалий активности:\")\n","print(anomalies[['ts', 'geo_city_id', 'event_count', 'z_score', 'activity_ratio', 'ratio_change']].head())\n","\n","import pandas as pd\n","from datetime import timedelta\n","\n","def detect_suspicious_ips(df: pd.DataFrame,\n","                         max_users_per_ip: int = 10) -> pd.DataFrame:\n","    \"\"\"\n","    Обнаруживает IP-адреса с аномально большим количеством пользователей\n","\n","    Параметры:\n","        df: DataFrame с данными\n","        max_users_per_ip: максимальное допустимое количество пользователей с одного IP\n","\n","    Возвращает:\n","        DataFrame с подозрительными IP и статистикой\n","    \"\"\"\n","    ip_stats = (\n","        df.groupby('ip')\n","        .agg(\n","            unique_users=('randPAS_user_passport_id', 'nunique'),\n","            total_actions=('randPAS_user_passport_id', 'count'),\n","            first_seen=('ts', 'min'),\n","            last_seen=('ts', 'max')\n","        )\n","        .reset_index()\n","    )\n","\n","    suspicious_ips = ip_stats[ip_stats['unique_users'] > max_users_per_ip]\n","    suspicious_ips['activity_period'] = suspicious_ips['last_seen'] - suspicious_ips['first_seen']\n","\n","    return suspicious_ips.sort_values('unique_users', ascending=False)\n","\n","\n","def detect_user_activity_spikes(df: pd.DataFrame,\n","                              time_window_sec: int = 60,\n","                              max_actions: int = 30) -> pd.DataFrame:\n","    \"\"\"\n","    Обнаруживает пользователей с аномально высокой активностью\n","\n","    Параметры:\n","        df: DataFrame с данными\n","        time_window_sec: временное окно в секундах для анализа\n","        max_actions: максимальное допустимое количество действий за окно\n","\n","    Возвращает:\n","        DataFrame с подозрительными пользователями и статистикой\n","    \"\"\"\n","\n","    df_sorted = df.sort_values(['randPAS_user_passport_id', 'ts'])\n","    df_sorted['time_diff'] = (\n","        df_sorted.groupby('randPAS_user_passport_id')['ts']\n","        .diff()\n","        .dt.total_seconds()\n","    )\n","    rapid_actions = df_sorted[df_sorted['time_diff'] < time_window_sec] \\\n","    .groupby('randPAS_user_passport_id') \\\n","    .agg(\n","        rapid_actions_count=('time_diff', 'count'),\n","        min_time_diff=('time_diff', 'min'),\n","        avg_time_diff=('time_diff', 'mean'),\n","        ip_list=('ip', lambda x: x.unique().tolist())\n","    ) \\\n","    .reset_index()\n","    suspicious_users = rapid_actions[rapid_actions['rapid_actions_count'] > max_actions]\n","    suspicious_users['ip_count'] = suspicious_users['ip_list'].apply(len)\n","\n","    return suspicious_users.sort_values('rapid_actions_count', ascending=False)\n","\n","\n","\n","print(\"Анализ подозрительных IP-адресов:\")\n","suspicious_ips = detect_suspicious_ips(df, max_users_per_ip=10)\n","print(suspicious_ips)\n","\n","print(\"\\nАнализ подозрительной активности пользователей:\")\n","suspicious_users = detect_user_activity_spikes(df, time_window_sec=10, max_actions=5)\n","print(suspicious_users)\n","\n","def detect_anomalous_time_windows(df, time_col, user_col, threshold=1.5, window_size='1H'):\n","    df = df.copy()\n","\n","    df[time_col] = pd.to_datetime(df[time_col])\n","    df['time_window'] = df[time_col].dt.floor(window_size)\n","\n","    df = df.sort_values(by=[user_col, time_col])\n","    df['time_diff'] = df.groupby(user_col)[time_col].diff().dt.total_seconds()\n","\n","    df['time_category'] = pd.cut(df['time_diff'],\n","                                 bins=[0, 30, 300, 1800, float('inf')],\n","                                 labels=['short', 'medium', 'long', 'very_long'])\n","\n","    time_window_stats = df.groupby('time_window')['time_category'].value_counts(normalize=True).unstack().fillna(0)\n","\n","    time_window_stats['short_ratio_change'] = time_window_stats['short'].pct_change().abs().fillna(0)\n","\n","    anomalous_windows = time_window_stats[time_window_stats['short_ratio_change'] > threshold]\n","\n","    return anomalous_windows[['short', 'medium', 'long']]\n","\n","# смотрим доли действий, разница между которыми до 30 секунд, до 5 минут и до 30 минут. если резко меняется, то выводим.\n","anomalies = detect_anomalous_time_windows(df, time_col='ts', user_col='randPAS_user_passport_id')\n","\n","print(\"Аномальные временные окна с резкими изменениями долей:\")\n","print(anomalies)\n","import pandas as pd\n","\n","def detect_anomalous_device_shares(df, time_col, device_col, threshold=1.5, window_size='30T'):\n","    df = df.copy()\n","    df[time_col] = pd.to_datetime(df[time_col])\n","    df['time_window'] = df[time_col].dt.floor(window_size)\n","\n","    device_shares = df.groupby(['time_window', device_col]).size().unstack().fillna(0)\n","    device_shares = device_shares.div(device_shares.sum(axis=1), axis=0)\n","    device_shares_change = device_shares.pct_change().abs().fillna(0)\n","\n","    anomalous_windows = device_shares_change[device_shares_change.max(axis=1) > threshold]\n","    return anomalous_windows\n","\n","#аномалии во временных окнах по распределению типов устройств\n","anomalous_device_windows = detect_anomalous_device_shares(df, time_col='ts', device_col='ua_device_type')\n","print(\"Аномальные временные окна с резкими изменениями в долях устройств:\")\n","print(anomalous_device_windows)\n","\n","def detect_anomalous_page_views(df, time_col, user_col, page_col, threshold=3, window_size='30T'):\n","    df = df.copy()\n","    df[time_col] = pd.to_datetime(df[time_col])\n","    df['time_window'] = df[time_col].dt.floor(window_size)\n","    page_views = df.groupby(['time_window', page_col])[user_col].nunique().unstack().fillna(0)\n","    page_views_change = page_views.pct_change().abs()\n","    page_views_change[page_views.shift(1) == 0] = np.nan\n","    anomalies = page_views_change[page_views_change > threshold].stack().reset_index()\n","    anomalies.columns = ['time_window', 'url', 'growth']\n","\n","    return anomalies.dropna()\n","\n","anomalous_page_windows = detect_anomalous_page_views(\n","    df,\n","    time_col='ts',\n","    user_col='randPAS_user_passport_id',\n","    page_col='url'\n",")\n","\n","if not anomalous_page_windows.empty:\n","    print(\" Найдены аномальные всплески посещаемости страниц:\")\n","    for _, row in anomalous_page_windows.iterrows():\n","        print(f\" {row['time_window']} |  {row['url']} |  Рост в {row['growth']:.2f} раз\")\n","else:\n","    print(\"Аномалий не найдено\")\n","\n","\n","from sklearn.ensemble import IsolationForest\n","import pandas as pd\n","\n","def detect_anomalous_users(df, user_col, time_col, page_col):\n","    user_page_times = df.groupby([user_col, page_col])[time_col].sum().reset_index()\n","\n","    user_avg_times = user_page_times.groupby(user_col)[time_col].mean().reset_index()\n","    user_avg_times.columns = [user_col, 'avg_time_spent']\n","    model = IsolationForest(contamination=0.05, random_state=42)  # contamination - доля аномальных\n","    user_avg_times['is_anomalous'] = model.fit_predict(user_avg_times[['avg_time_spent']])\n","    anomalous_users = user_avg_times[user_avg_times['is_anomalous'] == -1]\n","\n","    return anomalous_users\n","\n","\n","anomalous_users = detect_anomalous_users(df, user_col='randPAS_user_passport_id', time_col='secs', page_col='url')\n","\n","if not anomalous_users.empty:\n","    print(\"Найдены аномальные пользователи с нетипичным поведением:\")\n","    print(anomalous_users)\n","else:\n","    print(\"Аномальных пользователей не найдено\")\n","\n","     import numpy as np\n","from scipy import stats\n","from sklearn.ensemble import IsolationForest\n","from sklearn.neighbors import LocalOutlierFactor\n","\n","def zscore_detector(data: np.ndarray, threshold: float = 3.0) -> np.ndarray:\n","    \"\"\"\n","    Обнаружение аномалий с помощью Z-Score.\n","\n","    Параметры:\n","        data: одномерный массив данных\n","        threshold: пороговое значение (в стандартных отклонениях)\n","\n","    Возвращает:\n","        Бинарный массив (1 - аномалия, 0 - норма)\n","    \"\"\"\n","    z_scores = np.abs(stats.zscore(data))\n","    return (z_scores > threshold).astype(int)\n","\n","def iqr_detector(data: np.ndarray, k: float = 1.5) -> np.ndarray:\n","    \"\"\"\n","    Обнаружение аномалий с помощью межквартильного размаха (IQR).\n","\n","    Параметры:\n","        data: одномерный массив данных\n","        k: множитель IQR (обычно 1.5)\n","\n","    Возвращает:\n","        Бинарный массив (1 - аномалия, 0 - норма)\n","    \"\"\"\n","    q1 = np.percentile(data, 25)\n","    q3 = np.percentile(data, 75)\n","    iqr = q3 - q1\n","    lower_bound = q1 - k * iqr\n","    upper_bound = q3 + k * iqr\n","    return ((data < lower_bound) | (data > upper_bound)).astype(int)\n","\n","def modified_zscore_detector(data: np.ndarray, threshold: float = 3.5) -> np.ndarray:\n","    \"\"\"\n","    Обнаружение аномалий с помощью модифицированного Z-Score (более устойчив к выбросам).\n","\n","    Параметры:\n","        data: одномерный массив данных\n","        threshold: пороговое значение\n","\n","    Возвращает:\n","        Бинарный массив (1 - аномалия, 0 - норма)\n","    \"\"\"\n","    median = np.median(data)\n","    mad = np.median(np.abs(data - median))\n","    modified_z = 0.6745 * (data - median) / mad\n","    return (np.abs(modified_z) > threshold).astype(int)\n","\n","def isolation_forest_detector(data: np.ndarray,\n","                            contamination: float = 0.05) -> np.ndarray:\n","    \"\"\"\n","    Обнаружение аномалий с помощью Isolation Forest.\n","\n","    Параметры:\n","        data: одномерный массив данных\n","        contamination: предполагаемая доля аномалий\n","\n","    Возвращает:\n","        Бинарный массив (1 - аномалия, 0 - норма)\n","    \"\"\"\n","    clf = IsolationForest(contamination=contamination, random_state=42)\n","    data_reshaped = data.reshape(-1, 1)\n","    preds = clf.fit_predict(data_reshaped)\n","    return (preds == -1).astype(int)\n","\n","def lof_detector(data: np.ndarray,\n","                n_neighbors: int = 20,\n","                contamination: float = 0.05) -> np.ndarray:\n","    \"\"\"\n","    Обнаружение аномалий с помощью Local Outlier Factor (LOF).\n","\n","    Параметры:\n","        data: одномерный массив данных\n","        n_neighbors: количество соседей для анализа\n","        contamination: предполагаемая доля аномалий\n","\n","    Возвращает:\n","        Бинарный массив (1 - аномалия, 0 - норма)\n","    \"\"\"\n","    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n","    data_reshaped = data.reshape(-1, 1)\n","    preds = lof.fit_predict(data_reshaped)\n","    return (preds == -1).astype(int)\n","\n","def percentile_detector(data: np.ndarray,\n","                       lower_percentile: float = 1,\n","                       upper_percentile: float = 99) -> np.ndarray:\n","    \"\"\"\n","    Обнаружение аномалий по перцентилям.\n","\n","    Параметры:\n","        data: одномерный массив данных\n","        lower_percentile: нижний перцентиль\n","        upper_percentile: верхний перцентиль\n","\n","    Возвращает:\n","        Бинарный массив (1 - аномалия, 0 - норма)\n","    \"\"\"\n","    lower_bound = np.percentile(data, lower_percentile)\n","    upper_bound = np.percentile(data, upper_percentile)\n","    return ((data < lower_bound) | (data > upper_bound)).astype(int)\n","\n","import numpy as np\n","\n","def majority_anomaly_vote(*anomaly_arrays: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Объединяет результаты нескольких методов обнаружения аномалий.\n","    Аномалия отмечается, если более половины методов считают точку аномальной.\n","\n","    Параметры:\n","        *anomaly_arrays: любое количество бинарных массивов (0 - норма, 1 - аномалия)\n","\n","    Возвращает:\n","        Бинарный массив (1 - аномалия, 0 - норма)\n","    \"\"\"\n","    if len(anomaly_arrays) == 0:\n","        raise ValueError(\"Не передано ни одного массива аномалий.\")\n","    anomaly_matrix = np.vstack(anomaly_arrays)\n","    anomaly_counts = np.sum(anomaly_matrix, axis=0)\n","    threshold = len(anomaly_arrays) / 2\n","    final_anomalies = (anomaly_counts > threshold).astype(int)\n","\n","    return final_anomalies\n","\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def plot_anomalies_comparison(data, **anomaly_dict):\n","    \"\"\"\n","    Функция для построения графика сравнения методов обнаружения аномалий.\n","\n","    Параметры:\n","        data: массив исходных данных\n","        anomaly_dict: именованные массивы аномалий (где 1 - аномалия, 0 - нет)\n","    \"\"\"\n","    plt.figure(figsize=(12, 6))\n","    plt.scatter(range(len(data)), data, c='blue', label='Данные', alpha=0.5)\n","\n","    # Перебираем методы и отображаем найденные аномалии\n","    for method_name, anomalies in anomaly_dict.items():\n","        anomaly_indices = np.where(anomalies == 1)[0]\n","        plt.scatter(anomaly_indices, data[anomaly_indices], label=method_name, alpha=0.6)\n","\n","    plt.legend()\n","    plt.title(\"Сравнение методов обнаружения аномалий\")\n","    plt.show()\n","if __name__ == \"__main__\":\n","    # Создаем тестовые данные\n","    np.random.seed(42)\n","    normal_data = np.random.normal(0, 1, 1000)\n","    anomalies = np.array([5, -5, 10, -8, 12])  # Явные аномалии\n","    data = np.concatenate([normal_data, anomalies])\n","\n","    # Применяем детекторы\n","    methods = {\n","        \"Z-Score\": zscore_detector,\n","        \"IQR\": iqr_detector,\n","        \"Modified Z-Score\": modified_zscore_detector,\n","        \"Isolation Forest\": isolation_forest_detector,\n","        \"LOF\": lof_detector,\n","        \"Percentile\": percentile_detector\n","    }\n","\n","    results = {name: detector(data) for name, detector in methods.items()}\n","\n","    # Выводим количество обнаруженных аномалий\n","    print(\"Количество обнаруженных аномалий:\")\n","    for name, anomalies in results.items():\n","        print(f\"{name}: {np.sum(anomalies)}\")\n","\n","    # Визуализация с новой функцией\n","    plot_anomalies_comparison(data, **results)\n","\n","\n","    res = []\n","    for i, (name, anomalies) in enumerate(results.items(), 1):\n","        res.append(anomalies)\n","\n","    print(len(res))\n","    final_anomalies = majority_anomaly_vote(res[0], res[1], res[2], res[3], res[4], res[5])\n","\n","    print(\"Обнаружено аномалий (по большинству методов):\", np.sum(final_anomalies))\n","\n","\n",""]}]}